{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e6056-36a7-4549-bfb5-3571f695337b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# 配置参数 - 添加了测试集数据路径\n",
    "class Config:\n",
    "    train_data_path = \"C:/Users/YK/Desktop/liver_train\"  # 训练集路径\n",
    "    test_data_path = \"C:/Users/YK/Desktop/liver_test\"    # 测试集路径\n",
    "    mask_data_path = \"C:/Users/YK/Desktop/liver-mask_pngs\"\n",
    "    output_dir = \"./outputs(LF-liver)\"\n",
    "    seed = 42\n",
    "    img_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_workers = 0  # Windows系统下设置为0\n",
    "    num_epochs = 120\n",
    "    lr = 1e-4\n",
    "    num_classes = 3\n",
    "    val_size = 0.1   # 只保留验证集比例，移除测试集比例\n",
    "    weight_decay = 1e-5\n",
    "    n_bootstrap = 1000  # 用于计算置信区间的bootstrap抽样次数\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.seed)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(f\"{Config.output_dir}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/train\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/test\", exist_ok=True)\n",
    "\n",
    "# 自定义数据集类 - 保持不变\n",
    "class SWEDataset(Dataset):\n",
    "    def __init__(self, raw_path, mask_path, filenames, transform=None):\n",
    "        self.raw_path = raw_path\n",
    "        self.mask_path = mask_path\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.label_map = {'F2': 0, 'F3': 1, 'F4': 2}  # 根据实际标签修改\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载原始图像和掩膜\n",
    "        filename = self.filenames[idx]\n",
    "        raw_img = cv2.cvtColor(cv2.imread(f\"{self.raw_path}/{filename}\"), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f\"{self.mask_path}/{filename}\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # 应用掩膜\n",
    "        masked_img = cv2.bitwise_and(raw_img, raw_img, mask=mask)\n",
    "\n",
    "        # 针对性预处理：去除噪声、增强对比度\n",
    "        blurred = cv2.GaussianBlur(masked_img, (5, 5), 0)\n",
    "        \n",
    "        # 转换为LAB颜色空间进行CLAHE增强\n",
    "        lab = cv2.cvtColor(blurred, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        limg = cv2.merge((cl, a, b))\n",
    "        enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        # 调整大小\n",
    "        resized_img = cv2.resize(enhanced_img, Config.img_size)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=resized_img)\n",
    "            final_img = augmented['image']\n",
    "        else:\n",
    "            # 默认转换\n",
    "            final_img = transforms.ToTensor()(resized_img)\n",
    "            final_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(final_img)\n",
    "\n",
    "        # 提取标签 (假设文件名格式为\"F0_001.jpg\")\n",
    "        label_key = filename.split('_')[0]\n",
    "        if label_key not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label '{label_key}' in filename: {filename}\")\n",
    "        \n",
    "        label = self.label_map[label_key]\n",
    "\n",
    "        return final_img, label\n",
    "\n",
    "# 数据增强（使用 albumentations）- 保持不变\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# 加载训练集文件\n",
    "train_files = [f for f in os.listdir(Config.train_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 加载测试集文件\n",
    "test_files = [f for f in os.listdir(Config.test_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 检查文件数量\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the training data path\")\n",
    "\n",
    "if len(test_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the test data path\")\n",
    "\n",
    "# 从训练集中划分验证集\n",
    "train_labels = [f.split('_')[0] for f in train_files]\n",
    "train_files, val_files = train_test_split(train_files, test_size=Config.val_size, \n",
    "                                         random_state=Config.seed, stratify=train_labels)\n",
    "\n",
    "print(f\"Dataset sizes: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
    "\n",
    "# 处理数据不平衡 - 保持不变\n",
    "train_labels = [SWEDataset(Config.train_data_path, Config.mask_data_path, train_files).label_map[f.split('_')[0]] for f in train_files]\n",
    "class_counts = Counter(train_labels)\n",
    "print(f\"Class distribution in training set: {class_counts}\")\n",
    "\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# 创建数据加载器 - 修改部分：使用不同的路径\n",
    "train_dataset = SWEDataset(Config.train_data_path, Config.mask_data_path, train_files, transform=train_transform)\n",
    "val_dataset = SWEDataset(Config.train_data_path, Config.mask_data_path, val_files, transform=val_transform)\n",
    "test_dataset = SWEDataset(Config.test_data_path, Config.mask_data_path, test_files, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, sampler=sampler, \n",
    "                          num_workers=Config.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                        num_workers=Config.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                         num_workers=Config.num_workers, pin_memory=True)\n",
    "\n",
    "# 模型定义：使用efficientnet_b4 - 保持不变\n",
    "class SWEClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        # 加载预训练的EfficientNet-B4\n",
    "        self.backbone = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "        \n",
    "        # 获取分类器的输入特征数\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        # 替换分类器\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# 计算评估指标 - 保持不变\n",
    "def calculate_metrics(labels, preds, probs, class_idx, num_classes):\n",
    "    # 转换为二分类问题\n",
    "    binary_labels = (np.array(labels) == class_idx).astype(int)\n",
    "    binary_preds = (np.array(preds) == class_idx).astype(int)\n",
    "    \n",
    "    # 如果所有样本都属于同一类别，则无法计算ROC\n",
    "    if len(np.unique(binary_labels)) < 2:\n",
    "        return {\n",
    "            'sensitivity': 0.0,\n",
    "            'specificity': 0.0,\n",
    "            'ppv': 0.0,\n",
    "            'npv': 0.0,\n",
    "            'lr_pos': 0.0,\n",
    "            'lr_neg': 0.0,\n",
    "            'auc': 0.0,\n",
    "            'auc_ci_lower': 0.0,\n",
    "            'auc_ci_upper': 0.0\n",
    "        }\n",
    "    \n",
    "    binary_probs = np.array(probs)[:, class_idx]\n",
    "    \n",
    "    # 计算混淆矩阵元素\n",
    "    cm = confusion_matrix(binary_labels, binary_preds)\n",
    "    if cm.size == 1:  # 只有一个类别的情况\n",
    "        if binary_labels.sum() == 0:  # 所有样本都是负类\n",
    "            tn = cm[0, 0]\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = 0\n",
    "        else:  # 所有样本都是正类\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = cm[0, 0]\n",
    "    else:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # 计算各项指标\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    lr_pos = sensitivity / (1 - specificity) if (1 - specificity) > 0 else float('inf')\n",
    "    lr_neg = (1 - sensitivity) / specificity if specificity > 0 else float('inf')\n",
    "    \n",
    "    # 计算AUC及其95%置信区间\n",
    "    try:\n",
    "        auc_score = roc_auc_score(binary_labels, binary_probs)\n",
    "        \n",
    "        # 使用bootstrap方法计算95%置信区间\n",
    "        n_bootstraps = Config.n_bootstrap\n",
    "        bootstrapped_scores = []\n",
    "        \n",
    "        rng = np.random.RandomState(Config.seed)\n",
    "        for i in range(n_bootstraps):\n",
    "            # 采样索引\n",
    "            indices = rng.randint(0, len(binary_probs), len(binary_probs))\n",
    "            \n",
    "            # 检查引导样本中是否有两个类别\n",
    "            if len(np.unique(binary_labels[indices])) < 2:\n",
    "                continue\n",
    "            \n",
    "            score = roc_auc_score(binary_labels[indices], binary_probs[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "        \n",
    "        if bootstrapped_scores:\n",
    "            sorted_scores = np.array(bootstrapped_scores)\n",
    "            sorted_scores.sort()\n",
    "            confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "            confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "        else:\n",
    "            confidence_lower = confidence_upper = auc_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AUC for class {class_idx}: {str(e)}\")\n",
    "        auc_score = 0.0\n",
    "        confidence_lower = 0.0\n",
    "        confidence_upper = 0.0\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'lr_pos': lr_pos,\n",
    "        'lr_neg': lr_neg,\n",
    "        'auc': auc_score,\n",
    "        'auc_ci_lower': confidence_lower,\n",
    "        'auc_ci_upper': confidence_upper\n",
    "    }\n",
    "\n",
    "# 评估模型并计算所有指标 - 保持不变\n",
    "def evaluate_model_full(model, data_loader, dataset_name, device, num_classes):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # 计算总体准确率\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{dataset_name} Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # 计算每个类别的详细指标\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        class_metrics = calculate_metrics(all_labels, all_preds, all_probs, i, num_classes)\n",
    "        metrics[i] = class_metrics\n",
    "        \n",
    "        print(f\"\\n{dataset_name} Class {i} Metrics:\")\n",
    "        print(f\"Sensitivity (Recall): {class_metrics['sensitivity']:.4f}\")\n",
    "        print(f\"Specificity: {class_metrics['specificity']:.4f}\")\n",
    "        print(f\"PPV (Precision): {class_metrics['ppv']:.4f}\")\n",
    "        print(f\"NPV: {class_metrics['npv']:.4f}\")\n",
    "        print(f\"LR+: {class_metrics['lr_pos']:.4f}\")\n",
    "        print(f\"LR-: {class_metrics['lr_neg']:.4f}\")\n",
    "        print(f\"AUC: {class_metrics['auc']:.4f} (95% CI: {class_metrics['auc_ci_lower']:.4f}-{class_metrics['auc_ci_upper']:.4f})\")\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        binary_labels = (np.array(all_labels) == i).astype(int)\n",
    "        binary_probs = np.array(all_probs)[:, i]\n",
    "        \n",
    "        # 检查是否至少有两个类别\n",
    "        if len(np.unique(binary_labels)) < 2:\n",
    "            print(f\"Skipping ROC for class {i} - only one class present\")\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, _ = roc_curve(binary_labels, binary_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[i % len(colors)],\n",
    "                 lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{Config.output_dir}/results/{dataset_name.lower()}_roc_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, accuracy, cm, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# 检查是否有可用的 GPU - 保持不变\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SWEClassifier(num_classes=Config.num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# 打印模型结构 - 保持不变\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# 优化器和损失函数 - 保持不变\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# 训练函数 - 保持不变\n",
    "def train_model():\n",
    "    best_acc = 0.0\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "    patience = 20  # 早停机制的耐心值\n",
    "    no_improvement_epochs = 0  # 记录没有提升的周期数\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{Config.num_epochs}')\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # 更新进度条\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "        # 计算训练指标\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        train_acc.append(epoch_train_acc)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 计算验证指标\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        val_acc.append(epoch_val_acc)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(epoch_val_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': epoch_val_acc,\n",
    "            }, f\"{Config.output_dir}/models/best_model.pth\")\n",
    "            no_improvement_epochs = 0  # 验证集准确率提升，重置计数器\n",
    "            print(f\"New best model saved with val accuracy: {best_acc:.4f}\")\n",
    "        else:\n",
    "            no_improvement_epochs += 1  # 验证集准确率未提升，计数器加1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{Config.num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # 早停机制\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.output_dir}/results/training_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "# 执行训练与评估 - 保持不变\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"开始训练模型...\")\n",
    "    train_loss, train_acc, val_loss, val_acc = train_model()\n",
    "    \n",
    "    print(\"\\n加载最佳模型...\")\n",
    "    checkpoint = torch.load(f\"{Config.output_dir}/models/best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n评估训练集性能...\")\n",
    "    train_metrics, train_accuracy, train_cm, train_labels, train_probs = evaluate_model_full(\n",
    "        model, train_loader, \"Training Set\", device, Config.num_classes)\n",
    "    \n",
    "    print(\"\\n评估测试集性能...\")\n",
    "    test_metrics, test_accuracy, test_cm, test_labels, test_probs = evaluate_model_full(\n",
    "        model, test_loader, \"Test Set\", device, Config.num_classes)\n",
    "    \n",
    "    # 保存评估结果到文件\n",
    "    with open(f\"{Config.output_dir}/results/metrics_summary.txt\", 'w') as f:\n",
    "        f.write(\"模型评估结果汇总\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"训练集准确率: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"测试集准确率: {test_accuracy:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"训练集混淆矩阵:\\n\")\n",
    "        f.write(f\"{train_cm}\\n\\n\")\n",
    "        \n",
    "        f.write(\"测试集混淆矩阵:\\n\")\n",
    "        f.write(f\"{test_cm}\\n\\n\")\n",
    "        \n",
    "        for i in range(Config.num_classes):\n",
    "            f.write(f\"类别 {i} (F{i+2}) 指标\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\")\n",
    "            \n",
    "            f.write(\"训练集:\\n\")\n",
    "            f.write(f\"Sensitivity: {train_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {train_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {train_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {train_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {train_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {train_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {train_metrics[i]['auc']:.4f} (95% CI: {train_metrics[i]['auc_ci_lower']:.4f}-{train_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "            \n",
    "            f.write(\"测试集:\\n\")\n",
    "            f.write(f\"Sensitivity: {test_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {test_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {test_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {test_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {test_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {test_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {test_metrics[i]['auc']:.4f} (95% CI: {test_metrics[i]['auc_ci_lower']:.4f}-{test_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "\n",
    "    # 保存分类结果（二分类格式，适用于ROC曲线绘制）\n",
    "    for i in range(Config.num_classes):\n",
    "        # 创建当前类别的二分类标签（1为当前类别，0为其他类别）\n",
    "        train_binary_labels = (train_labels == i).astype(int)\n",
    "        test_binary_labels = (test_labels == i).astype(int)\n",
    "    \n",
    "        # 保存训练集结果\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_binary_true.npy\", train_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_proba.npy\", train_probs[:, i])\n",
    "    \n",
    "        # 保存测试集结果\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_binary_true.npy\", test_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_proba.npy\", test_probs[:, i])\n",
    "        \n",
    "    print(\"\\n模型训练和评估完成!\")\n",
    "    print(f\"结果已保存至: {Config.output_dir}/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df634966-df0b-490e-bcc6-9e1b9cdcee14",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "\n",
    "# 配置参数 - 添加了测试集数据路径\n",
    "class Config:\n",
    "    train_data_path = \"C:/Users/YK/Desktop/spleen_train\"  # 训练集路径\n",
    "    test_data_path = \"C:/Users/YK/Desktop/spleen_test\"    # 测试集路径\n",
    "    mask_data_path = \"C:/Users/YK/Desktop/spleen-mask_pngs\"\n",
    "    output_dir = \"./outputs(LF-spleen)\"\n",
    "    seed = 42\n",
    "    img_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_workers = 0  # Windows系统下设置为0\n",
    "    num_epochs = 120\n",
    "    lr = 1e-4\n",
    "    num_classes = 3\n",
    "    val_size = 0.1   # 只保留验证集比例，移除测试集比例\n",
    "    weight_decay = 1e-5\n",
    "    n_bootstrap = 1000  # 用于计算置信区间的bootstrap抽样次数\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.seed)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(f\"{Config.output_dir}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/train\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/test\", exist_ok=True)\n",
    "\n",
    "# 自定义数据集类 - 保持不变\n",
    "class SWEDataset(Dataset):\n",
    "    def __init__(self, raw_path, mask_path, filenames, transform=None):\n",
    "        self.raw_path = raw_path\n",
    "        self.mask_path = mask_path\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.label_map = {'F2': 0, 'F3': 1, 'F4': 2}  # 根据实际标签修改\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载原始图像和掩膜\n",
    "        filename = self.filenames[idx]\n",
    "        raw_img = cv2.cvtColor(cv2.imread(f\"{self.raw_path}/{filename}\"), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(f\"{self.mask_path}/{filename}\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        # 应用掩膜\n",
    "        masked_img = cv2.bitwise_and(raw_img, raw_img, mask=mask)\n",
    "\n",
    "        # 针对性预处理：去除噪声、增强对比度\n",
    "        blurred = cv2.GaussianBlur(masked_img, (5, 5), 0)\n",
    "        \n",
    "        # 转换为LAB颜色空间进行CLAHE增强\n",
    "        lab = cv2.cvtColor(blurred, cv2.COLOR_RGB2LAB)\n",
    "        l, a, b = cv2.split(lab)\n",
    "        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "        cl = clahe.apply(l)\n",
    "        limg = cv2.merge((cl, a, b))\n",
    "        enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "        # 调整大小\n",
    "        resized_img = cv2.resize(enhanced_img, Config.img_size)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=resized_img)\n",
    "            final_img = augmented['image']\n",
    "        else:\n",
    "            # 默认转换\n",
    "            final_img = transforms.ToTensor()(resized_img)\n",
    "            final_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(final_img)\n",
    "\n",
    "        # 提取标签 (假设文件名格式为\"F0_001.jpg\")\n",
    "        label_key = filename.split('_')[0]\n",
    "        if label_key not in self.label_map:\n",
    "            raise ValueError(f\"Unknown label '{label_key}' in filename: {filename}\")\n",
    "        \n",
    "        label = self.label_map[label_key]\n",
    "\n",
    "        return final_img, label\n",
    "\n",
    "# 数据增强（使用 albumentations）- 保持不变\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# 加载训练集文件\n",
    "train_files = [f for f in os.listdir(Config.train_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 加载测试集文件\n",
    "test_files = [f for f in os.listdir(Config.test_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 检查文件数量\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the training data path\")\n",
    "\n",
    "if len(test_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the test data path\")\n",
    "\n",
    "# 从训练集中划分验证集\n",
    "train_labels = [f.split('_')[0] for f in train_files]\n",
    "train_files, val_files = train_test_split(train_files, test_size=Config.val_size, \n",
    "                                         random_state=Config.seed, stratify=train_labels)\n",
    "\n",
    "print(f\"Dataset sizes: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
    "\n",
    "# 处理数据不平衡 - 保持不变\n",
    "train_labels = [SWEDataset(Config.train_data_path, Config.mask_data_path, train_files).label_map[f.split('_')[0]] for f in train_files]\n",
    "class_counts = Counter(train_labels)\n",
    "print(f\"Class distribution in training set: {class_counts}\")\n",
    "\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# 创建数据加载器 - 修改部分：使用不同的路径\n",
    "train_dataset = SWEDataset(Config.train_data_path, Config.mask_data_path, train_files, transform=train_transform)\n",
    "val_dataset = SWEDataset(Config.train_data_path, Config.mask_data_path, val_files, transform=val_transform)\n",
    "test_dataset = SWEDataset(Config.test_data_path, Config.mask_data_path, test_files, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, sampler=sampler, \n",
    "                          num_workers=Config.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                        num_workers=Config.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                         num_workers=Config.num_workers, pin_memory=True)\n",
    "\n",
    "# 模型定义：使用efficientnet_b4 - 保持不变\n",
    "class SWEClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        # 加载预训练的EfficientNet-B4\n",
    "        self.backbone = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "        \n",
    "        # 获取分类器的输入特征数\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        \n",
    "        # 替换分类器\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# 计算评估指标 - 保持不变\n",
    "def calculate_metrics(labels, preds, probs, class_idx, num_classes):\n",
    "    # 转换为二分类问题\n",
    "    binary_labels = (np.array(labels) == class_idx).astype(int)\n",
    "    binary_preds = (np.array(preds) == class_idx).astype(int)\n",
    "    \n",
    "    # 如果所有样本都属于同一类别，则无法计算ROC\n",
    "    if len(np.unique(binary_labels)) < 2:\n",
    "        return {\n",
    "            'sensitivity': 0.0,\n",
    "            'specificity': 0.0,\n",
    "            'ppv': 0.0,\n",
    "            'npv': 0.0,\n",
    "            'lr_pos': 0.0,\n",
    "            'lr_neg': 0.0,\n",
    "            'auc': 0.0,\n",
    "            'auc_ci_lower': 0.0,\n",
    "            'auc_ci_upper': 0.0\n",
    "        }\n",
    "    \n",
    "    binary_probs = np.array(probs)[:, class_idx]\n",
    "    \n",
    "    # 计算混淆矩阵元素\n",
    "    cm = confusion_matrix(binary_labels, binary_preds)\n",
    "    if cm.size == 1:  # 只有一个类别的情况\n",
    "        if binary_labels.sum() == 0:  # 所有样本都是负类\n",
    "            tn = cm[0, 0]\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = 0\n",
    "        else:  # 所有样本都是正类\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = cm[0, 0]\n",
    "    else:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # 计算各项指标\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    lr_pos = sensitivity / (1 - specificity) if (1 - specificity) > 0 else float('inf')\n",
    "    lr_neg = (1 - sensitivity) / specificity if specificity > 0 else float('inf')\n",
    "    \n",
    "    # 计算AUC及其95%置信区间\n",
    "    try:\n",
    "        auc_score = roc_auc_score(binary_labels, binary_probs)\n",
    "        \n",
    "        # 使用bootstrap方法计算95%置信区间\n",
    "        n_bootstraps = Config.n_bootstrap\n",
    "        bootstrapped_scores = []\n",
    "        \n",
    "        rng = np.random.RandomState(Config.seed)\n",
    "        for i in range(n_bootstraps):\n",
    "            # 采样索引\n",
    "            indices = rng.randint(0, len(binary_probs), len(binary_probs))\n",
    "            \n",
    "            # 检查引导样本中是否有两个类别\n",
    "            if len(np.unique(binary_labels[indices])) < 2:\n",
    "                continue\n",
    "            \n",
    "            score = roc_auc_score(binary_labels[indices], binary_probs[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "        \n",
    "        if bootstrapped_scores:\n",
    "            sorted_scores = np.array(bootstrapped_scores)\n",
    "            sorted_scores.sort()\n",
    "            confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "            confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "        else:\n",
    "            confidence_lower = confidence_upper = auc_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AUC for class {class_idx}: {str(e)}\")\n",
    "        auc_score = 0.0\n",
    "        confidence_lower = 0.0\n",
    "        confidence_upper = 0.0\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'lr_pos': lr_pos,\n",
    "        'lr_neg': lr_neg,\n",
    "        'auc': auc_score,\n",
    "        'auc_ci_lower': confidence_lower,\n",
    "        'auc_ci_upper': confidence_upper\n",
    "    }\n",
    "\n",
    "# 评估模型并计算所有指标 - 保持不变\n",
    "def evaluate_model_full(model, data_loader, dataset_name, device, num_classes):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # 计算总体准确率\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{dataset_name} Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # 计算每个类别的详细指标\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        class_metrics = calculate_metrics(all_labels, all_preds, all_probs, i, num_classes)\n",
    "        metrics[i] = class_metrics\n",
    "        \n",
    "        print(f\"\\n{dataset_name} Class {i} Metrics:\")\n",
    "        print(f\"Sensitivity (Recall): {class_metrics['sensitivity']:.4f}\")\n",
    "        print(f\"Specificity: {class_metrics['specificity']:.4f}\")\n",
    "        print(f\"PPV (Precision): {class_metrics['ppv']:.4f}\")\n",
    "        print(f\"NPV: {class_metrics['npv']:.4f}\")\n",
    "        print(f\"LR+: {class_metrics['lr_pos']:.4f}\")\n",
    "        print(f\"LR-: {class_metrics['lr_neg']:.4f}\")\n",
    "        print(f\"AUC: {class_metrics['auc']:.4f} (95% CI: {class_metrics['auc_ci_lower']:.4f}-{class_metrics['auc_ci_upper']:.4f})\")\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        binary_labels = (np.array(all_labels) == i).astype(int)\n",
    "        binary_probs = np.array(all_probs)[:, i]\n",
    "        \n",
    "        # 检查是否至少有两个类别\n",
    "        if len(np.unique(binary_labels)) < 2:\n",
    "            print(f\"Skipping ROC for class {i} - only one class present\")\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, _ = roc_curve(binary_labels, binary_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[i % len(colors)],\n",
    "                 lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{Config.output_dir}/results/{dataset_name.lower()}_roc_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, accuracy, cm, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# 检查是否有可用的 GPU - 保持不变\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SWEClassifier(num_classes=Config.num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# 打印模型结构 - 保持不变\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# 优化器和损失函数 - 保持不变\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# 训练函数 - 保持不变\n",
    "def train_model():\n",
    "    best_acc = 0.0\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "    patience = 20  # 早停机制的耐心值\n",
    "    no_improvement_epochs = 0  # 记录没有提升的周期数\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{Config.num_epochs}')\n",
    "        for inputs, labels in loop:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # 更新进度条\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "        # 计算训练指标\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        train_acc.append(epoch_train_acc)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 计算验证指标\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        val_acc.append(epoch_val_acc)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(epoch_val_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': epoch_val_acc,\n",
    "            }, f\"{Config.output_dir}/models/best_model.pth\")\n",
    "            no_improvement_epochs = 0  # 验证集准确率提升，重置计数器\n",
    "            print(f\"New best model saved with val accuracy: {best_acc:.4f}\")\n",
    "        else:\n",
    "            no_improvement_epochs += 1  # 验证集准确率未提升，计数器加1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{Config.num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # 早停机制\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.output_dir}/results/training_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "# 执行训练与评估 - 保持不变\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"开始训练模型...\")\n",
    "    train_loss, train_acc, val_loss, val_acc = train_model()\n",
    "    \n",
    "    print(\"\\n加载最佳模型...\")\n",
    "    checkpoint = torch.load(f\"{Config.output_dir}/models/best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n评估训练集性能...\")\n",
    "    train_metrics, train_accuracy, train_cm, train_labels, train_probs = evaluate_model_full(\n",
    "        model, train_loader, \"Training Set\", device, Config.num_classes)\n",
    "    \n",
    "    print(\"\\n评估测试集性能...\")\n",
    "    test_metrics, test_accuracy, test_cm, test_labels, test_probs = evaluate_model_full(\n",
    "        model, test_loader, \"Test Set\", device, Config.num_classes)\n",
    "    \n",
    "    # 保存评估结果到文件\n",
    "    with open(f\"{Config.output_dir}/results/metrics_summary.txt\", 'w') as f:\n",
    "        f.write(\"模型评估结果汇总\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"训练集准确率: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"测试集准确率: {test_accuracy:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"训练集混淆矩阵:\\n\")\n",
    "        f.write(f\"{train_cm}\\n\\n\")\n",
    "        \n",
    "        f.write(\"测试集混淆矩阵:\\n\")\n",
    "        f.write(f\"{test_cm}\\n\\n\")\n",
    "        \n",
    "        for i in range(Config.num_classes):\n",
    "            f.write(f\"类别 {i} (F{i+2}) 指标\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\")\n",
    "            \n",
    "            f.write(\"训练集:\\n\")\n",
    "            f.write(f\"Sensitivity: {train_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {train_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {train_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {train_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {train_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {train_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {train_metrics[i]['auc']:.4f} (95% CI: {train_metrics[i]['auc_ci_lower']:.4f}-{train_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "            \n",
    "            f.write(\"测试集:\\n\")\n",
    "            f.write(f\"Sensitivity: {test_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {test_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {test_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {test_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {test_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {test_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {test_metrics[i]['auc']:.4f} (95% CI: {test_metrics[i]['auc_ci_lower']:.4f}-{test_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "\n",
    "    # 保存分类结果（二分类格式，适用于ROC曲线绘制）\n",
    "    for i in range(Config.num_classes):\n",
    "        # 创建当前类别的二分类标签（1为当前类别，0为其他类别）\n",
    "        train_binary_labels = (train_labels == i).astype(int)\n",
    "        test_binary_labels = (test_labels == i).astype(int)\n",
    "    \n",
    "        # 保存训练集结果\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_binary_true.npy\", train_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_proba.npy\", train_probs[:, i])\n",
    "    \n",
    "        # 保存测试集结果\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_binary_true.npy\", test_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_proba.npy\", test_probs[:, i])\n",
    "        \n",
    "    print(\"\\n模型训练和评估完成!\")\n",
    "    print(f\"结果已保存至: {Config.output_dir}/results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93d37c-4772-4ff9-8bb7-e223e2d29b7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score\n",
    "from sklearn.utils import resample\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from collections import Counter\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class Config:\n",
    "    # 训练集路径\n",
    "    train_liver_raw_data_path = \"C:/Users/YK/Desktop/train_liver\"\n",
    "    train_liver_mask_data_path = \"C:/Users/YK/Desktop/train_liver-mask_pngs\"\n",
    "    train_spleen_raw_data_path = \"C:/Users/YK/Desktop/train_spleen\"\n",
    "    train_spleen_mask_data_path = \"C:/Users/YK/Desktop/train_spleen-mask_pngs\"\n",
    "    \n",
    "    # 测试集路径\n",
    "    test_liver_raw_data_path = \"C:/Users/YK/Desktop/test_liver\"\n",
    "    test_liver_mask_data_path = \"C:/Users/YK/Desktop/test_liver-mask_pngs\"\n",
    "    test_spleen_raw_data_path = \"C:/Users/YK/Desktop/test_spleen\"\n",
    "    test_spleen_mask_data_path = \"C:/Users/YK/Desktop/test_spleen-mask_pngs\"\n",
    "    \n",
    "    output_dir = \"./outputs(LF-combine)\"\n",
    "    seed = 42\n",
    "    img_size = (224, 224)\n",
    "    batch_size = 16\n",
    "    num_workers = 0  # Windows下建议设置为0\n",
    "    num_epochs = 120\n",
    "    lr = 1e-4\n",
    "    num_classes = 3\n",
    "    val_size = 0.1  # 仅保留验证集比例（从训练集中划分）\n",
    "    weight_decay = 1e-5\n",
    "    n_bootstrap = 1000\n",
    "\n",
    "# 设置随机种子，确保结果可复现\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(Config.seed)\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(f\"{Config.output_dir}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/train\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/results/test\", exist_ok=True)\n",
    "os.makedirs(f\"{Config.output_dir}/grad_cam\", exist_ok=True)\n",
    "\n",
    "# 自定义数据集类\n",
    "class SWEDataset(Dataset):\n",
    "    def __init__(self, liver_raw_path, liver_mask_path, spleen_raw_path, spleen_mask_path, filenames, transform=None):\n",
    "        self.liver_raw_path = liver_raw_path\n",
    "        self.liver_mask_path = liver_mask_path\n",
    "        self.spleen_raw_path = spleen_raw_path\n",
    "        self.spleen_mask_path = spleen_mask_path\n",
    "        self.filenames = filenames\n",
    "        self.transform = transform\n",
    "        self.label_map = {'F2': 0, 'F3': 1, 'F4': 2}  # 根据实际标签修改\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加载肝脏原始图像和掩膜\n",
    "        filename = self.filenames[idx]\n",
    "        liver_raw_img = cv2.imread(f\"{self.liver_raw_path}/{filename}\")\n",
    "        liver_exists = liver_raw_img is not None\n",
    "        if liver_exists:\n",
    "            liver_raw_img = cv2.cvtColor(liver_raw_img, cv2.COLOR_BGR2RGB)\n",
    "            liver_mask = cv2.imread(f\"{self.liver_mask_path}/{filename}\", cv2.IMREAD_GRAYSCALE)\n",
    "            # 应用肝脏掩膜\n",
    "            liver_masked_img = cv2.bitwise_and(liver_raw_img, liver_raw_img, mask=liver_mask)\n",
    "            # 针对性预处理：去除噪声、增强对比度\n",
    "            liver_blurred = cv2.GaussianBlur(liver_masked_img, (5, 5), 0)\n",
    "            liver_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            liver_lab = cv2.cvtColor(liver_blurred, cv2.COLOR_RGB2LAB)\n",
    "            liver_l, liver_a, liver_b = cv2.split(liver_lab)\n",
    "            liver_cl = liver_clahe.apply(liver_l)\n",
    "            liver_limg = cv2.merge((liver_cl, liver_a, liver_b))\n",
    "            liver_masked_img = cv2.cvtColor(liver_limg, cv2.COLOR_LAB2RGB)\n",
    "            # 调整大小\n",
    "            liver_masked_img = cv2.resize(liver_masked_img, Config.img_size)\n",
    "        else:\n",
    "            liver_masked_img = np.zeros((Config.img_size[0], Config.img_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        # 加载脾脏原始图像和掩膜\n",
    "        spleen_img_path = f\"{self.spleen_raw_path}/{filename}\"\n",
    "        spleen_raw_img = cv2.imread(spleen_img_path)\n",
    "        spleen_exists = spleen_raw_img is not None\n",
    "        if spleen_exists:\n",
    "            spleen_raw_img = cv2.cvtColor(spleen_raw_img, cv2.COLOR_BGR2RGB)\n",
    "            spleen_mask = cv2.imread(f\"{self.spleen_mask_path}/{filename}\", cv2.IMREAD_GRAYSCALE)\n",
    "            # 应用脾脏掩膜\n",
    "            spleen_masked_img = cv2.bitwise_and(spleen_raw_img, spleen_raw_img, mask=spleen_mask)\n",
    "            # 针对性预处理：去除噪声、增强对比度\n",
    "            spleen_blurred = cv2.GaussianBlur(spleen_masked_img, (5, 5), 0)\n",
    "            spleen_clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "            spleen_lab = cv2.cvtColor(spleen_blurred, cv2.COLOR_RGB2LAB)\n",
    "            spleen_l, spleen_a, spleen_b = cv2.split(spleen_lab)\n",
    "            spleen_cl = spleen_clahe.apply(spleen_l)\n",
    "            spleen_limg = cv2.merge((spleen_cl, spleen_a, spleen_b))\n",
    "            spleen_masked_img = cv2.cvtColor(spleen_limg, cv2.COLOR_LAB2RGB)\n",
    "            # 调整大小\n",
    "            spleen_masked_img = cv2.resize(spleen_masked_img, Config.img_size)\n",
    "        else:\n",
    "            spleen_masked_img = np.zeros((Config.img_size[0], Config.img_size[1], 3), dtype=np.uint8)\n",
    "\n",
    "        if self.transform:\n",
    "            liver_augmented = self.transform(image=liver_masked_img)\n",
    "            liver_masked_img = liver_augmented['image']\n",
    "            spleen_augmented = self.transform(image=spleen_masked_img)\n",
    "            spleen_masked_img = spleen_augmented['image']\n",
    "        else:\n",
    "            liver_masked_img = transforms.ToTensor()(liver_masked_img)\n",
    "            liver_masked_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(liver_masked_img)\n",
    "            spleen_masked_img = transforms.ToTensor()(spleen_masked_img)\n",
    "            spleen_masked_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(spleen_masked_img)\n",
    "\n",
    "        # 提取标签 (假设文件名格式为\"F0_001.jpg\")\n",
    "        label = self.label_map[filename.split('_')[0]]\n",
    "\n",
    "        return liver_masked_img, spleen_masked_img, label, liver_exists, spleen_exists\n",
    "\n",
    "# 数据增强（使用 albumentations）\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# 加载所有文件名并划分数据集\n",
    "# 从训练集肝脏数据路径加载文件\n",
    "train_files = [f for f in os.listdir(Config.train_liver_raw_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 从测试集肝脏数据路径加载文件\n",
    "test_files = [f for f in os.listdir(Config.test_liver_raw_data_path) \n",
    "             if f.endswith(('.png', '.jpg', '.jpeg')) and \n",
    "             f.split('_')[0] in ['F2', 'F3', 'F4']]  # 确保只包含有效标签的文件\n",
    "\n",
    "# 检查文件数量\n",
    "if len(train_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the training liver raw data path\")\n",
    "\n",
    "if len(test_files) == 0:\n",
    "    raise ValueError(\"No valid image files found in the test liver raw data path\")\n",
    "\n",
    "# 从训练集中划分验证集\n",
    "train_labels = [f.split('_')[0] for f in train_files]\n",
    "train_files, val_files = train_test_split(train_files, test_size=Config.val_size, \n",
    "                                         random_state=Config.seed, stratify=train_labels)\n",
    "\n",
    "print(f\"Dataset sizes: Train={len(train_files)}, Val={len(val_files)}, Test={len(test_files)}\")\n",
    "\n",
    "# 处理数据不平衡\n",
    "train_labels = [SWEDataset(Config.train_liver_raw_data_path, Config.train_liver_mask_data_path, \n",
    "                          Config.train_spleen_raw_data_path, Config.train_spleen_mask_data_path, \n",
    "                          train_files).label_map[f.split('_')[0]] for f in train_files]\n",
    "class_counts = Counter(train_labels)\n",
    "print(f\"Class distribution in training set: {class_counts}\")\n",
    "\n",
    "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "sample_weights = [class_weights[label] for label in train_labels]\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = SWEDataset(Config.train_liver_raw_data_path, Config.train_liver_mask_data_path, \n",
    "                          Config.train_spleen_raw_data_path, Config.train_spleen_mask_data_path, \n",
    "                          train_files, transform=train_transform)\n",
    "val_dataset = SWEDataset(Config.train_liver_raw_data_path, Config.train_liver_mask_data_path, \n",
    "                         Config.train_spleen_raw_data_path, Config.train_spleen_mask_data_path, \n",
    "                         val_files, transform=val_transform)\n",
    "test_dataset = SWEDataset(Config.test_liver_raw_data_path, Config.test_liver_mask_data_path, \n",
    "                         Config.test_spleen_raw_data_path, Config.test_spleen_mask_data_path, \n",
    "                         test_files, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, sampler=sampler, \n",
    "                          num_workers=Config.num_workers, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                        num_workers=Config.num_workers, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False, \n",
    "                         num_workers=Config.num_workers, pin_memory=True)\n",
    "\n",
    "# SE模块\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "# 模型定义：使用efficientnet_b4\n",
    "class SWEClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        # 肝脏分支\n",
    "        self.liver_backbone = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "        # 使用完整的特征提取器（包括最后一层）\n",
    "        self.liver_features = self.liver_backbone.features\n",
    "        self.liver_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.liver_se = SELayer(1792)  # EfficientNet-B4最终特征图通道数为1792\n",
    "        \n",
    "        # 脾脏分支\n",
    "        self.spleen_backbone = models.efficientnet_b4(weights=models.EfficientNet_B4_Weights.DEFAULT)\n",
    "        self.spleen_features = self.spleen_backbone.features\n",
    "        self.spleen_avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.spleen_se = SELayer(1792)\n",
    "        \n",
    "        # 分类器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1792 * 2, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, liver_x, spleen_x, liver_exists, spleen_exists):\n",
    "        # 肝脏特征提取\n",
    "        liver_x = self.liver_features(liver_x)\n",
    "        liver_x = self.liver_avgpool(liver_x)\n",
    "        liver_x = self.liver_se(liver_x)\n",
    "        liver_x = torch.flatten(liver_x, 1)\n",
    "        \n",
    "        # 脾脏特征提取\n",
    "        spleen_x = self.spleen_features(spleen_x)\n",
    "        spleen_x = self.spleen_avgpool(spleen_x)\n",
    "        spleen_x = self.spleen_se(spleen_x)\n",
    "        spleen_x = torch.flatten(spleen_x, 1)\n",
    "        \n",
    "        # 动态权重调整\n",
    "        liver_weight = liver_exists.float().unsqueeze(1).to(liver_x.device)\n",
    "        spleen_weight = spleen_exists.float().unsqueeze(1).to(spleen_x.device)\n",
    "        \n",
    "        # 应用存在性权重\n",
    "        liver_x = liver_x * liver_weight\n",
    "        spleen_x = spleen_x * spleen_weight\n",
    "        \n",
    "        # 拼接特征\n",
    "        combined_x = torch.cat((liver_x, spleen_x), dim=1)\n",
    "        \n",
    "        return self.classifier(combined_x)\n",
    "\n",
    "# 计算评估指标\n",
    "def calculate_metrics(labels, preds, probs, class_idx, num_classes):\n",
    "    # 转换为二分类问题\n",
    "    binary_labels = (np.array(labels) == class_idx).astype(int)\n",
    "    binary_preds = (np.array(preds) == class_idx).astype(int)\n",
    "    \n",
    "    # 如果所有样本都属于同一类别，则无法计算ROC\n",
    "    if len(np.unique(binary_labels)) < 2:\n",
    "        return {\n",
    "            'sensitivity': 0.0,\n",
    "            'specificity': 0.0,\n",
    "            'ppv': 0.0,\n",
    "            'npv': 0.0,\n",
    "            'lr_pos': 0.0,\n",
    "            'lr_neg': 0.0,\n",
    "            'auc': 0.0,\n",
    "            'auc_ci_lower': 0.0,\n",
    "            'auc_ci_upper': 0.0\n",
    "        }\n",
    "    \n",
    "    binary_probs = np.array(probs)[:, class_idx]\n",
    "    \n",
    "    # 计算混淆矩阵元素\n",
    "    cm = confusion_matrix(binary_labels, binary_preds)\n",
    "    if cm.size == 1:  # 只有一个类别的情况\n",
    "        if binary_labels.sum() == 0:  # 所有样本都是负类\n",
    "            tn = cm[0, 0]\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = 0\n",
    "        else:  # 所有样本都是正类\n",
    "            tn = 0\n",
    "            fp = 0\n",
    "            fn = 0\n",
    "            tp = cm[0, 0]\n",
    "    else:\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # 计算各项指标\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "    lr_pos = sensitivity / (1 - specificity) if (1 - specificity) > 0 else float('inf')\n",
    "    lr_neg = (1 - sensitivity) / specificity if specificity > 0 else float('inf')\n",
    "    \n",
    "    # 计算AUC及其95%置信区间\n",
    "    try:\n",
    "        auc_score = roc_auc_score(binary_labels, binary_probs)\n",
    "        \n",
    "        # 使用bootstrap方法计算95%置信区间\n",
    "        n_bootstraps = Config.n_bootstrap\n",
    "        bootstrapped_scores = []\n",
    "        \n",
    "        rng = np.random.RandomState(Config.seed)\n",
    "        for i in range(n_bootstraps):\n",
    "            # 采样索引\n",
    "            indices = rng.randint(0, len(binary_probs), len(binary_probs))\n",
    "            \n",
    "            # 检查引导样本中是否有两个类别\n",
    "            if len(np.unique(binary_labels[indices])) < 2:\n",
    "                continue\n",
    "            \n",
    "            score = roc_auc_score(binary_labels[indices], binary_probs[indices])\n",
    "            bootstrapped_scores.append(score)\n",
    "        \n",
    "        if bootstrapped_scores:\n",
    "            sorted_scores = np.array(bootstrapped_scores)\n",
    "            sorted_scores.sort()\n",
    "            confidence_lower = sorted_scores[int(0.025 * len(sorted_scores))]\n",
    "            confidence_upper = sorted_scores[int(0.975 * len(sorted_scores))]\n",
    "        else:\n",
    "            confidence_lower = confidence_upper = auc_score\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating AUC for class {class_idx}: {str(e)}\")\n",
    "        auc_score = 0.0\n",
    "        confidence_lower = 0.0\n",
    "        confidence_upper = 0.0\n",
    "    \n",
    "    return {\n",
    "        'sensitivity': sensitivity,\n",
    "        'specificity': specificity,\n",
    "        'ppv': ppv,\n",
    "        'npv': npv,\n",
    "        'lr_pos': lr_pos,\n",
    "        'lr_neg': lr_neg,\n",
    "        'auc': auc_score,\n",
    "        'auc_ci_lower': confidence_lower,\n",
    "        'auc_ci_upper': confidence_upper\n",
    "    }\n",
    "\n",
    "# 评估模型并计算所有指标\n",
    "def evaluate_model_full(model, data_loader, dataset_name, device, num_classes):\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for liver_inputs, spleen_inputs, labels, liver_exists, spleen_exists in tqdm(\n",
    "            data_loader, desc=f\"Evaluating {dataset_name}\"):\n",
    "            \n",
    "            liver_inputs, spleen_inputs, labels = liver_inputs.to(device), spleen_inputs.to(device), labels.to(device)\n",
    "            liver_exists, spleen_exists = liver_exists.to(device), spleen_exists.to(device)\n",
    "            \n",
    "            outputs = model(liver_inputs, spleen_inputs, liver_exists, spleen_exists)\n",
    "            \n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    # 计算总体准确率\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=range(num_classes))\n",
    "    \n",
    "    print(f\"\\n{dataset_name} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"{dataset_name} Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # 计算每个类别的详细指标\n",
    "    metrics = {}\n",
    "    for i in range(num_classes):\n",
    "        class_metrics = calculate_metrics(all_labels, all_preds, all_probs, i, num_classes)\n",
    "        metrics[i] = class_metrics\n",
    "        \n",
    "        print(f\"\\n{dataset_name} Class {i} Metrics:\")\n",
    "        print(f\"Sensitivity (Recall): {class_metrics['sensitivity']:.4f}\")\n",
    "        print(f\"Specificity: {class_metrics['specificity']:.4f}\")\n",
    "        print(f\"PPV (Precision): {class_metrics['ppv']:.4f}\")\n",
    "        print(f\"NPV: {class_metrics['npv']:.4f}\")\n",
    "        print(f\"LR+: {class_metrics['lr_pos']:.4f}\")\n",
    "        print(f\"LR-: {class_metrics['lr_neg']:.4f}\")\n",
    "        print(f\"AUC: {class_metrics['auc']:.4f} (95% CI: {class_metrics['auc_ci_lower']:.4f}-{class_metrics['auc_ci_upper']:.4f})\")\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange']\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        binary_labels = (np.array(all_labels) == i).astype(int)\n",
    "        binary_probs = np.array(all_probs)[:, i]\n",
    "        \n",
    "        # 检查是否至少有两个类别\n",
    "        if len(np.unique(binary_labels)) < 2:\n",
    "            print(f\"Skipping ROC for class {i} - only one class present\")\n",
    "            continue\n",
    "            \n",
    "        fpr, tpr, _ = roc_curve(binary_labels, binary_probs)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        plt.plot(fpr, tpr, color=colors[i % len(colors)],\n",
    "                 lw=2, label=f'Class {i}')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('1 - Specificity')\n",
    "    plt.ylabel('Sensitivity')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{Config.output_dir}/results/{dataset_name.lower()}_roc_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return metrics, accuracy, cm, np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "# 检查是否有可用的 GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = SWEClassifier(num_classes=Config.num_classes)\n",
    "model = model.to(device)\n",
    "\n",
    "# 打印模型结构\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "\n",
    "# 优化器和损失函数\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=Config.weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# 训练函数\n",
    "def train_model():\n",
    "    best_acc = 0.0\n",
    "    train_loss, val_loss = [], []\n",
    "    train_acc, val_acc = [], []\n",
    "    patience = 30  # 早停机制的耐心值\n",
    "    no_improvement_epochs = 0  # 记录没有提升的周期数\n",
    "\n",
    "    for epoch in range(Config.num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        loop = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{Config.num_epochs}')\n",
    "        for liver_inputs, spleen_inputs, labels, liver_exists, spleen_exists in loop:\n",
    "            liver_inputs, spleen_inputs, labels = liver_inputs.to(device), spleen_inputs.to(device), labels.to(device)\n",
    "            liver_exists, spleen_exists = liver_exists.to(device), spleen_exists.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(liver_inputs, spleen_inputs, liver_exists, spleen_exists)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            running_loss += loss.item() * liver_inputs.size(0)\n",
    "            \n",
    "            # 更新进度条\n",
    "            loop.set_postfix(loss=loss.item(), acc=correct/total)\n",
    "\n",
    "        # 计算训练指标\n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_loss.append(epoch_train_loss)\n",
    "        train_acc.append(epoch_train_acc)\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for liver_inputs, spleen_inputs, labels, liver_exists, spleen_exists in val_loader:\n",
    "                liver_inputs, spleen_inputs, labels = liver_inputs.to(device), spleen_inputs.to(device), labels.to(device)\n",
    "                liver_exists, spleen_exists = liver_exists.to(device), spleen_exists.to(device)\n",
    "                outputs = model(liver_inputs, spleen_inputs, liver_exists, spleen_exists)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_running_loss += loss.item() * liver_inputs.size(0)\n",
    "\n",
    "        # 计算验证指标\n",
    "        epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        epoch_val_acc = val_correct / val_total\n",
    "        val_loss.append(epoch_val_loss)\n",
    "        val_acc.append(epoch_val_acc)\n",
    "        \n",
    "        # 更新学习率\n",
    "        scheduler.step(epoch_val_acc)\n",
    "\n",
    "        # 保存最佳模型\n",
    "        if epoch_val_acc > best_acc:\n",
    "            best_acc = epoch_val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_acc': epoch_val_acc,\n",
    "            }, f\"{Config.output_dir}/models/best_model.pth\")\n",
    "            no_improvement_epochs = 0  # 验证集准确率提升，重置计数器\n",
    "            print(f\"New best model saved with val accuracy: {best_acc:.4f}\")\n",
    "        else:\n",
    "            no_improvement_epochs += 1  # 验证集准确率未提升，计数器加1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{Config.num_epochs} | \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f} Acc: {epoch_train_acc:.4f} | \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f} Acc: {epoch_val_acc:.4f} | \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "        # 早停机制\n",
    "        if no_improvement_epochs >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation accuracy for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    # 绘制训练曲线\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_loss, label='Train Loss')\n",
    "    plt.plot(val_loss, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(train_acc, label='Train Accuracy')\n",
    "    plt.plot(val_acc, label='Val Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{Config.output_dir}/results/training_curve.tif\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "\n",
    "# 执行训练与评估\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"开始训练模型...\")\n",
    "    train_loss, train_acc, val_loss, val_acc = train_model()\n",
    "    \n",
    "    print(\"\\n加载最佳模型...\")\n",
    "    checkpoint = torch.load(f\"{Config.output_dir}/models/best_model.pth\")\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(\"\\n评估训练集性能...\")\n",
    "    train_metrics, train_accuracy, train_cm, train_labels, train_probs = evaluate_model_full(\n",
    "        model, train_loader, \"Training Set\", device, Config.num_classes)\n",
    "    \n",
    "    print(\"\\n评估测试集性能...\")\n",
    "    test_metrics, test_accuracy, test_cm, test_labels, test_probs = evaluate_model_full(\n",
    "        model, test_loader, \"Test Set\", device, Config.num_classes)\n",
    "    \n",
    "    # 保存评估结果到文件\n",
    "    with open(f\"{Config.output_dir}/results/metrics_summary.txt\", 'w') as f:\n",
    "        f.write(\"模型评估结果汇总\\n\")\n",
    "        f.write(\"=\"*50 + \"\\n\")\n",
    "        f.write(f\"训练集准确率: {train_accuracy:.4f}\\n\")\n",
    "        f.write(f\"测试集准确率: {test_accuracy:.4f}\\n\\n\")\n",
    "        \n",
    "        f.write(\"训练集混淆矩阵:\\n\")\n",
    "        f.write(f\"{train_cm}\\n\\n\")\n",
    "        \n",
    "        f.write(\"测试集混淆矩阵:\\n\")\n",
    "        f.write(f\"{test_cm}\\n\\n\")\n",
    "        \n",
    "        for i in range(Config.num_classes):\n",
    "            f.write(f\"类别 {i} (F{i+2}) 指标\\n\")\n",
    "            f.write(\"-\"*30 + \"\\n\")\n",
    "            \n",
    "            f.write(\"训练集:\\n\")\n",
    "            f.write(f\"Sensitivity: {train_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {train_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {train_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {train_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {train_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {train_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {train_metrics[i]['auc']:.4f} (95% CI: {train_metrics[i]['auc_ci_lower']:.4f}-{train_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "            \n",
    "            f.write(\"测试集:\\n\")\n",
    "            f.write(f\"Sensitivity: {test_metrics[i]['sensitivity']:.4f}\\n\")\n",
    "            f.write(f\"Specificity: {test_metrics[i]['specificity']:.4f}\\n\")\n",
    "            f.write(f\"PPV: {test_metrics[i]['ppv']:.4f}\\n\")\n",
    "            f.write(f\"NPV: {test_metrics[i]['npv']:.4f}\\n\")\n",
    "            f.write(f\"LR+: {test_metrics[i]['lr_pos']:.4f}\\n\")\n",
    "            f.write(f\"LR-: {test_metrics[i]['lr_neg']:.4f}\\n\")\n",
    "            f.write(f\"AUC: {test_metrics[i]['auc']:.4f} (95% CI: {test_metrics[i]['auc_ci_lower']:.4f}-{test_metrics[i]['auc_ci_upper']:.4f})\\n\\n\")\n",
    "    \n",
    "    # 保存分类结果（二分类格式，适用于ROC曲线绘制）\n",
    "    for i in range(Config.num_classes):\n",
    "        # 创建当前类别的二分类标签（1为当前类别，0为其他类别）\n",
    "        train_binary_labels = (train_labels == i).astype(int)\n",
    "        test_binary_labels = (test_labels == i).astype(int)\n",
    "    \n",
    "        # 保存训练集结果\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_binary_true.npy\", train_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/train/F{i+2}_proba.npy\", train_probs[:, i])\n",
    "    \n",
    "        # 保存测试集结果\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_binary_true.npy\", test_binary_labels)\n",
    "        np.save(f\"{Config.output_dir}/results/test/F{i+2}_proba.npy\", test_probs[:, i])\n",
    "        \n",
    "    print(\"\\n模型训练和评估完成!\")\n",
    "    print(f\"结果已保存至: {Config.output_dir}/results\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
